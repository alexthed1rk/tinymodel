{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a54f82d2-e8de-46be-afcf-391901c97707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import torch\n",
    "\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from torch import nn\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bca9e9f4-f669-406a-83b2-a7e5eb41b403",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = 'ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "MODEL_N_CHANNELS = 3 # RGB\n",
    "MODEL_N_CLASSES = len(MODEL_CLASSES)\n",
    "MODEL_HIDDEN_UNITS = 96\n",
    "MODEL_FILEPATH = Path('TinyVGG_P20_H96_64x64.pth')\n",
    "\n",
    "TRANSFORM_MARGIN_SIZE = 32\n",
    "TRANSFORM_DATA_SIZE = (64, 64)\n",
    "\n",
    "MODEL_TRANSFORM = v2.Compose([\n",
    "\tv2.Grayscale(num_output_channels=MODEL_N_CHANNELS),\n",
    "\tv2.Resize(size=TRANSFORM_DATA_SIZE),\n",
    "\tv2.ToImage(),\n",
    "\tv2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "IMAGE_FILEPATH = Path('IMG.JPG')\n",
    "IMAGE_CLASSES = 'АВГКМНЕ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b3b65f5b-fe5c-4947-a021-b8287619b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "\t\"\"\"\n",
    "\tModel architecture copying TinyVGG from: \n",
    "\thttps://poloclub.github.io/cnn-explainer/\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_channels: int, hidden_units: int, out_features: int) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.tinymodel = nn.Sequential(\n",
    "\t\t\t# Conv2d Block 1\n",
    "\t\t\tnn.Conv2d(in_channels, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\n",
    "\t\t\t# Conv2d Block 1\n",
    "\t\t\tnn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\n",
    "\t\t\t# Classifier Block\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(hidden_units*16*16, out_features)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\t\treturn self.tinymodel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8a9d055c-7a30-4620-8d86-8326ac5df99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_and_extract_letters(filepath=IMAGE_FILEPATH, margin=TRANSFORM_MARGIN_SIZE):\n",
    "\timage = cv.imread(filepath)\n",
    "\timage = cv.rotate(image, cv.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\timage = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\timage = cv.medianBlur(image, 3)\n",
    "\timage = cv.adaptiveThreshold(image, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, 31, 11)\n",
    "\n",
    "\tkernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (5, 5))\n",
    "\timage = cv.morphologyEx(image, cv.MORPH_CLOSE, kernel, iterations=3)\n",
    "\n",
    "\timage_contours, _ = cv.findContours(image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\timage_contours = sorted(image_contours, key=lambda image_contour: cv.boundingRect(image_contour)[0])\n",
    "\n",
    "\timage_letters = []\n",
    "\tfor image_contour in image_contours:\n",
    "\t\tx, y, width, height = cv.boundingRect(image_contour)\n",
    "\n",
    "\t\tif width < 80 or height < 80:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\timage_letter = image[y-margin:y+height+margin, x-margin:x+width+margin].copy()\n",
    "\t\timage_letter = cv.bitwise_not(image_letter)\n",
    "\t\timage_letters.append(Image.fromarray(image_letter))\n",
    "\n",
    "\treturn image_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a095b3f5-5520-4265-bd69-97aee6518a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_letters(model, transform, letters, image_classes, model_classes):\n",
    "\tmodel.eval()\n",
    "\twith torch.inference_mode():\n",
    "\t\tfor letter_id, letter in enumerate(letters):\n",
    "\t\t\tletter_pred = model(transform(letter).unsqueeze(dim=0))\n",
    "\t\t\t\n",
    "\t\t\tletter_pred_class = torch.argmax(torch.softmax(letter_pred, dim=1), dim=1)\n",
    "\t\t\tletter_pred_class = model_classes[letter_pred_class]\n",
    "\t\t\t\n",
    "\t\t\tletter_class = image_classes[letter_id]\n",
    "\t\t\tprint(letter_class, '=', letter_pred_class, end=', ')\n",
    "\tprint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d1b12d2-2ddf-4999-a8e4-f71752445dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\tletters = read_image_and_extract_letters(IMAGE_FILEPATH)\n",
    "\tmodel = TinyVGG(MODEL_N_CHANNELS, MODEL_HIDDEN_UNITS, MODEL_N_CLASSES)\n",
    "\tmodel.load_state_dict(torch.load(f=MODEL_FILEPATH))\n",
    "\tpredict_image_letters(model, MODEL_TRANSFORM, letters, IMAGE_CLASSES, MODEL_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48556089-d622-4781-8ec5-7fb8a973391c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "А = А, В = В, Г = Г, К = К, М = М, Н = Н, Е = Е, \n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
