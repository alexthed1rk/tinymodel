{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36147025-83aa-4645-85fa-b5e42bc8a207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The MIT License (MIT)\n",
    "#\n",
    "# Copyright (c) 2026 Alexander Zhura\n",
    "#\n",
    "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
    "# of this software and associated documentation files (the \"Software\"), to deal\n",
    "# in the Software without restriction, including without limitation the rights\n",
    "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
    "# copies of the Software, and to permit persons to whom the Software is\n",
    "# furnished to do so, subject to the following conditions:\n",
    "#\n",
    "# The above copyright notice and this permission notice shall be included in\n",
    "# all copies or substantial portions of the Software.\n",
    "#\n",
    "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
    "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
    "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
    "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
    "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
    "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
    "# THE SOFTWARE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afd651e9-3d97-42d2-9fee-079430d59b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import random\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "from PIL import Image\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ed3053b-39d8-4c54-b28c-d0cb1299b2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_CLASSES = 'ЁАБВГДЕЖЗИЙКЛМНОПРСТУФХЦЧШЩЪЫЬЭЮЯ'\n",
    "MODEL_N_CHANNELS = 3 # RGB\n",
    "MODEL_N_CLASSES = len(MODEL_CLASSES)\n",
    "MODEL_HIDDEN_UNITS = 96\n",
    "MODEL_FILEPATH = Path('TinyVGG_P20_H96_64x64.pth')\n",
    "MODEL_DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DATASET_PATH = Path('dataset/test/')\n",
    "\n",
    "DATALOADER_BATCH_SIZE = 32\n",
    "DATALOADER_TRAIN_SIZE = 0.20\n",
    "\n",
    "TRANSFORM_MARGIN_SIZE = 32\n",
    "TRANSFORM_DATA_SIZE = (64, 64)\n",
    "\n",
    "MODEL_TRANSFORM = v2.Compose([\n",
    "\tv2.Grayscale(num_output_channels=MODEL_N_CHANNELS),\n",
    "\tv2.Resize(size=TRANSFORM_DATA_SIZE),\n",
    "\tv2.ToImage(),\n",
    "\tv2.ToDtype(torch.float32, scale=True)\n",
    "])\n",
    "\n",
    "IMAGE_FILEPATH = Path('IMG.JPG')\n",
    "IMAGE_CLASSES = 'АВГКМНЕ'\n",
    "IMAGE_N_CLASSES = len(IMAGE_CLASSES)\n",
    "\n",
    "RANDOM_SEED = 42\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if MODEL_DEVICE == 'cuda':\n",
    "\ttorch.cuda.manual_seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bd36d88-643e-4b1d-964d-bb49ed71c21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def walk_through_dataset(top):\n",
    "\tresult = dict()\n",
    "\tfor dirpath, _, filenames in top.walk():\n",
    "\t\tif dirpath.stem != top.stem:\n",
    "\t\t\tinfo = {'filepath': dirpath, 'n_images': len(filenames)}\n",
    "\t\t\tresult[dirpath.stem] = info\n",
    "\treturn result\n",
    "\n",
    "def dataset_train_test_split(dataset_paths=MODEL_DATASET_PATH, train_size=DATALOADER_TRAIN_SIZE, random_seed=RANDOM_SEED):\n",
    "\tdataset_walk_info = walk_through_dataset(dataset_paths)\n",
    "\tmodel_class_to_id = dict(zip(MODEL_CLASSES, range(MODEL_N_CLASSES)))\n",
    "\n",
    "\timages_train_target, images_train_data = [], []\n",
    "\timages_test_target, images_test_data = [], []\n",
    "\n",
    "\tfor image_class, image_dataset in dataset_walk_info.items():\n",
    "\t\timage_target = model_class_to_id[image_class]\n",
    "\t\n",
    "\t\tn_images = image_dataset['n_images']\n",
    "\t\tn_train_images = int(train_size*n_images)\n",
    "\t\n",
    "\t\trandom.seed(random_seed)\n",
    "\t\timage_dataset = list(image_dataset['filepath'].glob('*.jpeg'))\n",
    "\t\trandom.shuffle(image_dataset)\n",
    "\t\n",
    "\t\tfor n_image, filepath in enumerate(image_dataset):\n",
    "\t\t\timage_handle = Image.open(filepath)\n",
    "\t\t\timage = image_handle.copy()\n",
    "\t\t\timage_handle.close()\n",
    "\t\t\t\n",
    "\t\t\tif n_image <= n_train_images:\n",
    "\t\t\t\timages_train_target.append(image_target)\n",
    "\t\t\t\timages_train_data.append(image)\n",
    "\t\t\telse:\n",
    "\t\t\t\timages_test_target.append(image_target)\n",
    "\t\t\t\timages_test_data.append(image)\n",
    "\n",
    "\treturn images_train_data, images_test_data, images_train_target, images_test_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a469f24-8ce2-4cda-b950-095f47fa3a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "\tdef __init__(self, data, target, transform=None) -> None:\n",
    "\t\tself.data = data\n",
    "\t\tself.target = target\n",
    "\t\tself.transform = transform\n",
    "\n",
    "\tdef __len__(self) -> int:\n",
    "\t\treturn len(self.target)\n",
    "\n",
    "\tdef __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "\t\tdata_id = self.transform(self.data[index])\n",
    "\t\ttarget_id = self.target[index]\n",
    "\n",
    "\t\treturn data_id, target_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fb538f8-ee38-4f9a-b04c-cc71b3e28521",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_image_and_extract_letters(filepath=IMAGE_FILEPATH, margin=TRANSFORM_MARGIN_SIZE):\n",
    "\timage = cv.imread(filepath)\n",
    "\timage = cv.rotate(image, cv.ROTATE_90_COUNTERCLOCKWISE)\n",
    "\timage = cv.cvtColor(image, cv.COLOR_BGR2GRAY)\n",
    "\timage = cv.medianBlur(image, 3)\n",
    "\timage = cv.adaptiveThreshold(image, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY_INV, 31, 11)\n",
    "\n",
    "\tkernel = cv.getStructuringElement(cv.MORPH_ELLIPSE, (5, 5))\n",
    "\timage = cv.morphologyEx(image, cv.MORPH_CLOSE, kernel, iterations=3)\n",
    "\n",
    "\timage_contours, _ = cv.findContours(image, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "\timage_contours = sorted(image_contours, key=lambda image_contour: cv.boundingRect(image_contour)[0])\n",
    "\n",
    "\timage_letters = []\n",
    "\tfor image_contour in image_contours:\n",
    "\t\tx, y, width, height = cv.boundingRect(image_contour)\n",
    "\n",
    "\t\tif width < 80 or height < 80:\n",
    "\t\t\tcontinue\n",
    "\n",
    "\t\timage_letter = image[y-margin:y+height+margin, x-margin:x+width+margin].copy()\n",
    "\t\timage_letter = cv.bitwise_not(image_letter)\n",
    "\t\timage_letters.append(Image.fromarray(image_letter))\n",
    "\n",
    "\treturn image_letters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8652a43e-e877-4de1-9379-1fcf1b163ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyVGG(nn.Module):\n",
    "\t\"\"\"\n",
    "\tModel architecture copying TinyVGG from: \n",
    "\thttps://poloclub.github.io/cnn-explainer/\n",
    "\t\"\"\"\n",
    "\tdef __init__(self, in_channels: int, hidden_units: int, out_features: int) -> None:\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.tinymodel = nn.Sequential(\n",
    "\t\t\t# Conv2d Block 1\n",
    "\t\t\tnn.Conv2d(in_channels, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\n",
    "\t\t\t# Conv2d Block 1\n",
    "\t\t\tnn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.MaxPool2d(2),\n",
    "\n",
    "\t\t\t# Classifier Block\n",
    "\t\t\tnn.Flatten(),\n",
    "\t\t\tnn.Linear(hidden_units*16*16, out_features)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x: torch.Tensor):\n",
    "\t\treturn self.tinymodel(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd91fc0d-7fcc-45c2-8417-7e26ca8cfd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model: torch.nn.Module,\n",
    "               dataloader: torch.utils.data.DataLoader,\n",
    "               model_loss: torch.nn.Module,\n",
    "               model_optimizer: torch.optim.Optimizer):\n",
    "\n",
    "\tmodel.train()\n",
    "\ttrain_loss, train_acc = 0, 0\n",
    "    \n",
    "\tfor X, y in dataloader:\n",
    "\t\tX, y = X.to(MODEL_DEVICE), y.to(MODEL_DEVICE)\n",
    "\n",
    "\t\ty_pred = model(X)\n",
    "\t\tloss = model_loss(y_pred, y)\n",
    "\t\t\n",
    "\t\ttrain_loss += loss.item()\n",
    "\t\ty_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "\t\ttrain_acc += 100*(y_pred_class == y).sum().item()/len(y_pred)\n",
    "\t\t\n",
    "\t\tmodel_optimizer.zero_grad()\n",
    "\t\tloss.backward()\n",
    "\t\tmodel_optimizer.step()\n",
    "\n",
    "\ttrain_loss /= len(dataloader)\n",
    "\ttrain_acc /= len(dataloader)\n",
    "\t\n",
    "\treturn train_loss, train_acc\n",
    "\n",
    "def test_step(model: torch.nn.Module,\n",
    "              dataloader: torch.utils.data.DataLoader,\n",
    "              model_loss: torch.nn.Module,\n",
    "              model_optimizer):\n",
    "    \n",
    "\tmodel.eval()\n",
    "\ttest_loss, test_acc = 0, 0\n",
    "\n",
    "\twith torch.inference_mode(): \n",
    "\t\tfor X, y in dataloader:\n",
    "\t\t\tX, y = X.to(MODEL_DEVICE), y.to(MODEL_DEVICE)\n",
    "            \n",
    "\t\t\ty_pred = model(X)\n",
    "\t\t\tloss = model_loss(y_pred, y)\n",
    "\n",
    "\t\t\ttest_loss += loss.item()\n",
    "\t\t\ty_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n",
    "\t\t\ttest_acc += 100*(y_pred_class == y).sum().item()/len(y_pred)\n",
    "\n",
    "\t\ttest_loss /= len(dataloader)\n",
    "\t\ttest_acc /= len(dataloader)\n",
    "\n",
    "\t\treturn test_loss, test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d3ffd43-ea7b-4590-82b0-a90e3b544531",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_letters(model, transform, letters, image_classes, model_classes):\n",
    "\ttest_acc_list = []\n",
    "\tmodel.eval()\n",
    "\twith torch.inference_mode():\n",
    "\t\tfor letter_id, letter in enumerate(letters):\n",
    "\t\t\tletter_pred = model(transform(letter).unsqueeze(dim=0).to(MODEL_DEVICE))\n",
    "\t\t\t\n",
    "\t\t\tletter_pred_class = torch.argmax(torch.softmax(letter_pred, dim=1), dim=1)\n",
    "\t\t\tletter_pred_class = model_classes[letter_pred_class]\n",
    "\t\t\t\n",
    "\t\t\tletter_class = image_classes[letter_id]\n",
    "\t\t\tprint(letter_class, '=', letter_pred_class, end=', ')\n",
    "\t\t\ttest_acc_list.append(letter_class == letter_pred_class)\n",
    "\tprint()\n",
    "\treturn sum(test_acc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aacae020-0432-402e-a7f5-f80d7c85d3d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "\t# Train Test Split Raw Data\n",
    "\tIMAGES_TRAIN_DATA, IMAGES_TEST_DATA, IMAGES_TRAIN_TARGET, IMAGES_TEST_TARGET = dataset_train_test_split()\n",
    "\n",
    "\t# Init Custom Train Test Pytorch Datasets\n",
    "\tIMAGE_TRAIN_DATASET = ImageDataset(IMAGES_TRAIN_DATA, IMAGES_TRAIN_TARGET, MODEL_TRANSFORM)\n",
    "\tIMAGE_TEST_DATASET = ImageDataset(IMAGES_TEST_DATA, IMAGES_TEST_TARGET, MODEL_TRANSFORM)\n",
    "\n",
    "\t# Init Custom Train Test Pytorch Dataloaders\n",
    "\tIMAGE_TRAIN_DATALOADER = DataLoader(dataset=IMAGE_TRAIN_DATASET, batch_size=DATALOADER_BATCH_SIZE, shuffle=True)\n",
    "\tIMAGE_TEST_DATALOADER = DataLoader(dataset=IMAGE_TEST_DATASET, batch_size=DATALOADER_BATCH_SIZE)\n",
    "\n",
    "\t# Init Custom TinyVGG Multiclass CNN\n",
    "\tmodel = TinyVGG(in_channels=MODEL_N_CHANNELS, hidden_units=MODEL_HIDDEN_UNITS, out_features=MODEL_N_CLASSES).to(MODEL_DEVICE)\n",
    "\tmodel_loss = nn.CrossEntropyLoss()\n",
    "\tmodel_optimizer = torch.optim.AdamW(params=model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
    "\n",
    "\t# Init Given Image Letters\n",
    "\tletters = read_image_and_extract_letters(IMAGE_FILEPATH)\n",
    "\n",
    "\t# Run Train Test Loop\n",
    "\tepoch, result_acc = 0, 0\n",
    "\twhile result_acc != IMAGE_N_CLASSES:\n",
    "\t\ttrain_loss, train_acc = train_step(model, IMAGE_TRAIN_DATALOADER, model_loss, model_optimizer)\n",
    "\t\ttest_loss, test_acc = test_step(model, IMAGE_TEST_DATALOADER, model_loss, model_optimizer)\n",
    "\t\tresult_acc = predict_image_letters(model, MODEL_TRANSFORM, letters, IMAGE_CLASSES, MODEL_CLASSES)\n",
    "\t\tprint(f\"Epoch: {epoch:>2} | Train loss: {train_loss:.5f} | Train accuracy: {train_acc:.2f}%\", end=' | ')\n",
    "\t\tprint(f\"Test loss: {test_loss:.5f} | Test accuracy: {test_acc:.2f}% | Result accuaracy {result_acc} / {IMAGE_N_CLASSES}\")\n",
    "\t\tepoch += 1\n",
    "\n",
    "\t# Keep Model State\n",
    "\t# torch.save(obj=model.state_dict(), f=MODEL_FILEPATH)\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e09b3-cae6-4f69-8f82-4e1ec359a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "\tmain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
